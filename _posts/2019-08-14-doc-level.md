---
layout: posts
title: "Un-bottlenecking document-level machine translation"
published: false
---

This blog entry revisits our submission to the WMT19 translation shared task for English-German, probably one of the first competitive document-level MT system in the history of the WMT shared tasks. The main focus will be document-level neural machine translation with deep Transformer models. These models draw inspiration from recent advances in large-scale document-level language modeling rather than from previous work in document-level MT.

<strong>Context-bottleneck:</strong> Contrary to previous document-level MT work, we do not propose architecture changes to deal with added context, instead we simple translate between full documents as if they were sentences. 

<strong>Data-bottleneck:</strong> Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train Transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries and we will discuss the technical challenges that come with these large augmented data sets and deeper models.

<strong>Quality-bottleneck:</strong> We will touch upon the question of evaluating document-level MT systems and try to put it into context with the highly disputed human-parity question. Based on WMT19 human evaluation results, evaluators strongly prefer the document-level systems over our fully comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment, although there are open questions. It further turns out that the discussed system wildly outperforms previous systems on benchmarks for MT anaphora resolution, improving the state of the art on the ContraPro test set from ca. 60% to nearly 89%.

## Towards human parity on the WMT news translation task?

Since I work at Microsoft, you know that there is no way that I would not double down on the concept of human parity in machine translation.
A first attempt at proving human parity results for MT on the WMT news translation benchmark was [Hassan et al. (2018)](https://arxiv.org/abs/1803.05567){:target="_blank"} for the Chinese-English translation direction (full disclosure: I am a co-author of this paper). 
This work drew a considerable amount of criticism -- deserved and undeserved -- but by kicking the hornet's nest of human parity also sparked a useful and on-going discussion on the feasibility of human parity results in MT in general and weaknesses of current best-practice human evaluation methods during WMT specifically.

Among the papers that re-analyzed the published data from [Hassan et al. (2018)](https://arxiv.org/abs/1803.05567){:target="_blank"} two deserve special attention: [LÃ¤ubli et al. (2018)](https://aclweb.org/anthology/D18-1512/){:target="_blank"} and [Toral et al. (2018)](https://aclweb.org/anthology/W18-6312/){:target="_blank"}. While both papers largely confirmed the human parity results within the strong evaluation constraints posed by [Hassan et al. (2018)](https://arxiv.org/abs/1803.05567){:target="_blank"}, they also demonstrated that fundamental flaws in -- at that time -- widely accepted evaluation settings  invalidated the human parity claims. These flaws were:

* Low-quality reference translations;
* Use of non-expert judges during human evaluation;
* Use of Translationese on the source side of WMT test sets;
* Lack of document-context during human evaluation.

The organizers of the WMT19 shared-task on news translation [(Bojar et al. 2019)](http://www.statmt.org/wmt19/pdf/WMT0001.pdf){:target="_blank"}
tried to address these flaws by providing fully one-directional test sets with original language on the source side and switching from reference-based direct assessment () to source-based direct assessment (). For selected language pairs (en-cs, en-de and en-zh) a kind of document-level human evaluation procedure was introduced. 
On top of that, WMT19 provided subsets of training data with document boundaries for first time in its history. 

Not all issues have been addressed and the impact of document-level context on the human evaluation on WMT19 results is debatable. The questions of reference translation quality and non-expert judges remain largely unresolved and probably pose a serious obstacle for potential human parity claims. 
Nevertheless, the improved evaluation settings and added document-level training data allows the community to take another shot at the concept of human parity. Follow-up work then needs to analyze how wrong we are this time. I am quite sure this is already happening while I am writing this blog post.

My personal view on the topic of human parity is that we can probably get there relatively quickly once we figure out how to fool-proof our human evaluation procedure against in hindsight obvious points of criticism like the ones listed above. The fool-proofing part, however, might turn out to be really hard.

## Un-bottlenecking document-level machine translation

Shared-task veteran, occasion to brush-up on document-level MT, this blog post is part of the on-going effort. 



## Long-sequence training with deep transformer models

Now it gets technical. 

### Batch fitting

![Memory usage of single 16GB GPU]({{ "/assets/images/blog/gpumem.png" }})


| Max. length | 6 layers | 12 layers | 24 layers |
|:-----------:|:--------:|:---------:|:---------:|
| 10 | 526 | 303 | 135 |
| 20 | 261 | 150 | 66 |
| 50 | 102 |  58 | 26 |
| 100 | 49 |  28 | 12 |
| 200 | 23 |  12 | 5 |
| 500 | 7  |   4 | 1 |
| 1000 | 3 |   1 | -- |
{: .table .table-bordered .table-striped }

### Mixed-precision training


### Gradient-checkpointing

![Memory allocation during forward-backward steps]({{ "/assets/images/blog/memory-fwbw.gif" }})

![Memory allocation during forward-backward steps with checkpointing]({{ "/assets/images/blog/memory-fwbw-chkpt.gif" }})


We manually flag a checkpoint after each transformer block. In the future, we plan to implement an automatic search for suitable checkpoint locations.

![Memory usage of single 16GB GPU]({{ "/assets/images/blog/batchsize.png" }})


### Virtual batches instead of optimizer delay

Batch-size warm-up. 

### Depth-scaling for deep transformer models

$$ \frac{\partial}{\partial x}LN(l_i(x) + x) = \alpha \cdot W $$

## Document-level data augmentation

Previous work on document-level MT was also limited by the availability of document-level parallel data. This year, for a subset (Europarl, Rapid, News-Commentary) of the parallel data document boundaries have been restored, the rest is provided without boundaries. The available monolingual news crawl data contains document boundaries for all its content, both in German and English. All three types of data are assembled into real and fake documents with varying degrees of data augmentation.

### Preprocessing and document-level mark-up
We use given document boundaries to concatenate parallel sentences into document-level sequences; parallel documents consist of the same number of sentences on both sides. We want to ensure that the models produce as many output sentences per document as input sentences were provided when we simply break on predicted separators to revert back to the sentence-level for evaluation. As a fail-safe mechanism, we sentence-align the sentence-broken document-level output with a sentence-level translation. The sentence-level translation serves as a template in which we replace all 1-1-aligned sentences with their document-level counterparts. This mechanism proved useful for early or intermediate models. For all our submissions, the document-level systems would correctly predict sentence boundaries and the fail-safe could be skipped. This by itself is noteworthy.

The example below contains an example document from the validation set with added mark-up. We add symbols for document start (&lt;BEG&gt;) and end (&lt;END&gt;) and for sentence separators (&lt;SEP&gt;). In cases where documents exceed our length limit of 1000 sub-word tokens, we use a break symbol (&lt;BRK&gt;) instead of &lt;END&gt; and start the next sequence with a continuation symbol (&lt;CNT&gt;) instead of &lt;BEG&gt;. When breaking parallel documents due to the length restriction, we break consistently across languages. All training and validation data is marked up in the same way. 

<div style="text-align: justify; padding: 10px; background-color: #EEE; margin-left: 1cm; margin-right: 1cm; font-size: 80%"> &lt;BEG&gt; In 911 Call, Professor Admits to Shooting Girlfriend&lt;SEP&gt; 
In a 911 call, his voice only slightly shaky, college professor 
Shannon Lamb told police he had shot his girlfriend and officers 
needed to get over to their house.&lt;SEP&gt; Lamb made a point to say 
his "sweet dog" was there alive and probably upset, and said the 
dead woman's family contacts could be found on her phone.&lt;SEP&gt; 
Inside the home, officers found Amy Prentiss' body and a hand-written 
note scribbled on a white legal pad: "I am so very sorry I wish I 
could take it back I loved Amy and she is the only woman who ever 
loved me," read the letter authorities say was signed by Lamb.&lt;SEP&gt; 
There was no indication that Lamb, who was teaching two online 
classes for Delta State University in Cleveland, Mississippi, had 
already traveled 300 miles to the school's campus, where police 
believe he shot and killed a well-liked history professor, Ethan 
Schmidt, in the doorway to his office.&lt;SEP&gt; Delta State University 
police chief Lynn Buford said university officials heard about the 
shooting at 10:18 a.m.&lt;SEP&gt; He said Lamb made the fateful 911 call 
sometime after that.&lt;SEP&gt; By the end of the day, there would be one
more death: Lamb took his own life as police closed in on him.&lt;SEP&gt; 
A day after the school shooting forced students and faculty to hide 
behind locked doors, authorities were still trying to piece together 
what motivated Lamb.&lt;SEP&gt; The details released by investigators at 
both ends of the state as well as students and staff who knew him 
helped paint a picture of a talented but possibly troubled teacher. &lt;SEP&gt; Students said they looked forward to his class.&lt;SEP&gt; Police in
Gautier, where Prentiss died, said he had no history of violence or 
criminal record.&lt;SEP&gt; Schmidt himself had included Lamb in a book 
he wrote where he acknowledged the "wonderful people" he shared his 
academic life with.&lt;SEP&gt; Both taught in the Division of Social 
Sciences and History, which lists 17 faculty members, and many 
students took courses from both.&lt;SEP&gt; At the same time, there were
some inclinations of problems.&lt;SEP&gt;&lt;BRK&gt;
</div>

### Monolingual data with document boundaries
We back-translated the entire available news crawl data for our sentence-level system and can use the present boundaries to assemble parallel documents. Due to the large amount of monolingual data, we do not use any document-level data-augmentation besides back-translation. 


### Parallel data with document boundaries
In the case of original parallel data with document boundaries, we use all available content without data filtering. This set of original documents is quite small (about 200K documents) compared to the back-translated data, so we increase the size of the corpus by adding randomly chosen continuous parallel sub-documents to the original data set, but not more than 10 possible sub-document per full document. Allowing all possible sub-documents would heavily skew the distribution towards longer documents. We repeat the process until the size of the corpus matches about half the size of the back-translated data. Every repetition is created with different random sub-documents. 


### Parallel data without document boundaries
The majority of authentic parallel data does not come with documents boundaries. Here, we shuffle the filtered parallel sentences and randomly add document boundaries. This results in fake documents that consist of unrelated but parallel sentences with consistent sentence boundaries inside the documents. Again, we repeat the process with random shuffles resulting in new fake documents until we reach a size close to half of the back-translated data. 

## To BLEU or not to BLEU 

## WMT19 evaluation results

## The ContraPro test set

Saliency http://www.statmt.org/wmt19/pdf/WMT0069.pdf