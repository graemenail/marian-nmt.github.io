
.. _program_listing_file_src_training_deprecated_graph_group_multinode.h:

Program Listing for File graph_group_multinode.h
================================================

|exhale_lsh| :ref:`Return to documentation for file <file_src_training_deprecated_graph_group_multinode.h>` (``src/training/deprecated/graph_group_multinode.h``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   #pragma once
   
   #include "training/graph_group.h"
   #include "training/communicator.h"
   
   #include "common/filesystem.h"
   #include "3rd_party/threadpool.h"
   #include "training/graph_group.h"
   
   #include <condition_variable>
   #include <future>
   #include <thread>
   
   namespace marian {
   
   class MultiNodeGraphGroup : public MultiNodeGraphGroupBase {
     using Base = MultiNodeGraphGroupBase;
   public:
     virtual void setScheduler(Ptr<Scheduler> scheduler) override;
   
   private:
     // General variables.
   
     bool initialized_{false};
   
     std::vector<Ptr<TensorAllocator>> allocators_;
   
     // Client variables.
   
     ThreadPool* clientThreadPool_;
   
     std::mutex mutexClientInit_;
   
     std::mutex schedulerMutex_;
   
     size_t batchIter_ = 0;
   
     // Server (shard) variables.
   
     std::thread* serverShardThread_;
   
     std::vector<float> serverShardBufferCPU_;
   
     std::vector<Tensor> shardParams_;
   
     std::vector<Tensor> shardGrads_;
   
     std::vector<Ptr<OptimizerBase>> shardOptimizers_;
   
     std::vector<std::mutex> shardMutex_;
   
     // Communication variables.
   
     std::vector<size_t> nodeSizes_;
   
     std::vector<size_t> shardSizes_;
   
     std::vector<std::vector<float>> clientCommBuffersCPU_;
   
     static const int MPI_TAG_GRAD_PUSH_MSG_{0};
   
     static const int MPI_TAG_GRAD_PUSH_{5};
   
     static const int MPI_TAG_PARAM_PUSH_{10};
   
     static const unsigned int MSG_INFO_SIZE_{0}, MSG_INFO_CLIENT_{1},
         MSG_INFO_BATCHWORDS_{2}, MSG_INFO_STATUS_{3};
   
     static const unsigned int STATUS_NODE_TRAINING_{0}, STATUS_NODE_FINISHED_{1};
   
     bool clientCommOverlap;
   
     // Overlapping communication and computation variables.
   
     std::vector<std::thread*> clientCommThreads_;
   
     bool stopClientCommThreads_{false};
   
     std::vector<Tensor> clientSummedGradsGPU;
   
     std::vector<size_t> clientSummedWordCounts_;
   
     std::vector<size_t> clientCommittedWordCounts_;
   
     std::vector<Ptr<OptimizerBase>> clientLocalOptimizers_;
   
     std::vector<Tensor> clientCommOverlapBuffersGPU_;
   
     std::vector<bool> clientCommOverlapBuffersFilled_;
   
     std::vector<std::mutex> mutexClientCommOverlapBuffersFilled_;
   
     std::vector<std::condition_variable> cvClientCommOverlapBuffersFilled_;
   
     size_t tau_{1};
     std::vector<std::mutex> optDelayMutex_;
     std::vector<size_t> delay_count;
     std::vector<int> totalBatchWords;
     std::vector<Tensor> accGradients, accGradientBuffer;
   
     // bool useLocalOpt_;
   
     Tensor newTensor(int size, Ptr<Backend> backend);
   
     virtual void init(Ptr<data::Batch> batch);
   
     void setupClients(Ptr<data::Batch> batch);
   
     void runBatchThroughClientGraphs(Ptr<data::Batch> batch);
   
     void calculateNodeSizes();
   
     void initClientCpuBuffers();
   
     void initClientCommOverlapVars();
   
     void initClientCommOverlapGpuTensors();
   
     void setupServerShards();
   
     void calculateShardSizes();
   
     void initShardGpuTensors();
   
     virtual void launchServerThread();
   
     void shutDownServerThread();
   
     void launchCommOverlapThreads();
   
     void shutDownCommOverlapThreads();
   
     virtual void synchronizeWithServerShards(Tensor newGrads,
                                              Tensor oldParams,
                                              int gpu,
                                              size_t batchWords = 0);
   
     void execute(Ptr<data::Batch> batch);
   
     virtual void signalFinishedToServerShards();
   
   public:
     MultiNodeGraphGroup(Ptr<Options> options, Ptr<IMPIWrapper> mpi)
         : Base(options, mpi),
           clientCommOverlap{options_->get<bool>("multi-node-overlap")},
           tau_{(size_t)options_->get<double>("optimizer-delay")} { }
   
     virtual ~MultiNodeGraphGroup() {
       if(initialized_) {
         if(clientCommOverlap) {
           shutDownCommOverlapThreads();
         }
         signalFinishedToServerShards();  // notify other nodes that this node has
                                          // finished training
         shutDownServerThread();
       }
       delete clientThreadPool_;
     }
   
     void update(Ptr<data::Batch> batch) override {
       validate();
       // Only take batch assigned to this node
       if(batchIter_ % mpi_->numMPIProcesses() == (size_t)mpi_->myMPIRank()) {
         execute(batch);
       }
       batchIter_++;
     }
   
     void load() override {
       if(!options_->get<bool>("no-reload")) {
         std::string name = options_->get<std::string>("model");
   
         if(filesystem::exists(name)) {
           if(scheduler_)
             scheduler_->load(name);
           size_t i = 0;
           for(auto graph : clientGraphs_)
             clientBuilders_[i++]->load(graph, name);
         } else if(options_->hasAndNotEmpty("pretrained-model")) {
           std::string init = options_->get<std::string>("pretrained-model");
           LOG(info,
               "Initialize model weights with the pre-trained model {}",
               init);
           size_t i = 0;
           for(auto graph : clientGraphs_)
             clientBuilders_[i++]->load(graph, init, false);
         }
       }
     }
   
     void save(bool final = false) override { save(clientGraphs_[0], final); }
   
     void save(Ptr<ExpressionGraph> graph, bool final = false) {
       size_t idx = 0;
       for(size_t i = 0; i < clientGraphs_.size(); ++i) {
         if(graph == clientGraphs_[i]) {
           idx = i;
           break;
         }
       }
   
       if(options_->get<bool>("overwrite")) {
         std::string name = options_->get<std::string>("model");
   
         clientBuilders_[idx]->save(clientGraphs_[idx], name, true);
         if(scheduler_)
           scheduler_->save(name);
       } else {
         std::string name = options_->get<std::string>("model");
   
         if(!final) {
           std::string numberOfBatches
               = scheduler_ ? std::to_string(scheduler_->numberOfBatches())
                            : "unknown";
           std::string nameOverwrite = name;
           nameOverwrite.replace(
               name.size() - 4, 4, ".iter" + numberOfBatches + ".npz");
           clientBuilders_[idx]->save(clientGraphs_[idx], nameOverwrite);
         }
   
         clientBuilders_[idx]->save(clientGraphs_[idx], name, true);
         if(scheduler_)
           scheduler_->save(name);
       }
     }
   
     Ptr<data::BatchStats> collectStats(const std::vector<Ptr<Vocab>>& vocabs) {
       return GraphGroup::collectStats(clientGraphs_[0], clientBuilders_[0], vocabs);
     }
   };
   }  // namespace marian
