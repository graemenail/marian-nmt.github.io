---
layout: docs
title: MT Marathon 2018 
permalink: /examples/mtm2018-labs
icon: fa-cogs
---

## Introduction

**Marian** is an efficient Neural Machine Translation framework written in pure
C++ with minimal dependencies. It has mainly been developed at the Adam
Mickiewicz University in Pozna≈Ñ (AMU) and at the University of Edinburgh.
It is currently being deployed in multiple European and commercial projects.

Marian is also a Machine Translation Marathon 2016 project that is celebrating
its third birthday during the MTM 2019!

More information:
- [Features & benchmarks](/features)
- [Documentation](/docs)
- [FAQ](/faq)
- [Google discussion group](https://groups.google.com/forum/#!forum/marian-nmt)
- [GitHub issues](http://github.com/marian-nmt/marian-dev/issues)

In this tutorial we will learn how to do efficient neural machine translation
using the Marian toolkit by optimizing the speed, accuracy and use of resources
for training and decoding of NMT models.

No background knowledge is required, but if you are completely new to neural
machine translation and Marian, you may take a look at at [the introductory
tutorial to Marian]() first.


## Installation

There are two repositories that marian can be obtained from:
`marian-nmt/marian` and `marian-nmt/marian-dev`.  The former includes the
latest stable release of Marian, while the latter is our main development
repository with new features and improvements.


### Models and data

We need to download models and data that we will use for this tutorial:

```
```


## Training

### Model architecture

### GPU memory

### Fine-tuning


## Decoding

### Batched decoding

### Back-translation

### Optimizing parameters


## Teacher-student method

### Teacher model

### Student model

### CPU decoding


