<!--[if IE 8]> <html lang="en" class="ie8"> <![endif]-->
<!--[if IE 9]> <html lang="en" class="ie9"> <![endif]-->
<!--[if !IE]><!-->
<html lang="en">
<!--<![endif]-->

  <head>
  <title>
    
    Marian :: FAQ
    
  </title>
  <!-- Meta -->
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Fast Neural Machine Translation in C++">

  <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
  <!-- Global CSS -->
  <link rel="stylesheet" href="/assets/plugins/bootstrap/css/bootstrap.min.css">
  <!-- Plugins CSS -->
  <link rel="stylesheet" href="/assets/plugins/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/pygments/github.css">

  <!-- Theme CSS -->
  <link id="theme-style" rel="stylesheet" href="/assets/css/styles.css">
  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.css" />
  <!--[if lt IE 9]>
    <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.ie.css" />
  <![endif]-->

  
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  

  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109819276-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-109819276-1');
</script>

  

</head>


  <body class="body-blue">
    <a class="github-fork-ribbon" href="https://github.com/marian-nmt/marian" title="Fork me on GitHub">Fork me on GitHub</a>

    <div class="page-wrapper">

    <header id="header" class="header">
  <div class="container">
    <div class="branding">
      <h1 class="logo">
        <a href="/">
          <span aria-hidden="true" class="icon_documents_alt icon"></span>
          <span class="text-highlight">Marian</span><span class="text-bold">NMT</span>
        </a>
      </h1>
      <p class="description">Fast Neural Machine Translation in C++</p>
    </div><!--//branding-->

    <ol class="breadcrumb">


 

 

 

 

 

 

 

 

 

 

 

 

 

 
 <li>
   <a class="page-link" href="/quickstart/">Quick start</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/features/">Features &amp; Benchmarks</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/docs/">Documentation</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/examples/">Examples</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/faq">FAQ</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/publications/">Publications</a>
 </li>
 

</ol>


  </div><!--//container-->
</header><!--//header-->


    <div class="doc-wrapper">
      <div class="container">

        <div id="doc-header" class="doc-header text-center">
          <h1 class="doc-title">
            
            <i class="icon fa fa-life-ring }}"></i>
            
            FAQ
          </h1>
          <div class="meta">
            <i class="fa fa-clock-o"></i>
            Last updated: 24 November 2017
          </div>
        </div><!--//doc-header-->

        <div class="doc-body">
          <div class="doc-content">
            <div class="content-inner">

              <h2 id="answers">Answers</h2>

<h3 id="general">General</h3>

<h4 class="question" id="what-is-marian">What is Marian?</h4>
<p>Marian is a pure C++ neural machine translation toolkit. It supports training and translation of a number of popular NMT models. Underneath the NMT API lurkes
a quite mature deep learning engine with no external dependencies other than boost and Nvidia’s CUDA (with optional CUDNN for convolutional networks).</p>

<p>Due to its self-contained nature, it is quite easy to optimize Marian for NMT specific tasks which results in one of the more efficient NMT toolkits available.
Take a look at the <a href="/features/#benchmarks">benchmarks</a>.</p>

<h4 class="question" id="where-can-i-get-marian">Where can I get Marian?</h4>
<p>Follow the steps in <a href="/quickstart">quick start</a> to get install Marian from our github repository <a href="https://github.com/marian-nmt/marian">https://github.com/marian-nmt/marian</a></p>

<h4 class="question" id="where-can-i-get-support">Where can I get support?</h4>
<p>There is a Google discussion group available at <a href="https://groups.google.com/forum/#!forum/marian-nmt">https://groups.google.com/forum/#!forum/marian-nmt</a>.
You can send questions to the group by e-mail: <code class="highlighter-rouge">marian-nmt@googlegroups.com</code></p>

<h4 class="question" id="where-can-i-report-bugs">Where can I report bugs?</h4>
<p>I you believe you have encoutered a bug, please file an issue at <a href="https://github.com/marian-nmt/marian/issues">https://github.com/marian-nmt/marian/issues</a></p>

<h4 class="question" id="how-should-i-cite-marian">How should I cite Marian?</h4>
<p>Currently there is no proper publication for Marian yet (we are working on it!). For the moment please cite the following publication which
introduces Marian’s predecessor AmuNMT:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>@InProceedings{junczys2016neural,
  title     = {Is Neural Machine Translation Ready for Deployment? A Case Study
               on 30 Translation Directions},
  author    = {Junczys-Dowmunt, Marcin and Dwojak, Tomasz and Hoang, Hieu},
  booktitle = {Proceedings of the 9th International Workshop on Spoken Language
               Translation (IWSLT)},
  year      = {2016},
  address   = {Seattle, WA},
  url       = {http://workshop2016.iwslt.org/downloads/IWSLT_2016_paper_4.pdf}
}
</code></pre>
</div>
<p>There’s also a bunch of publications that use Marian on our <a href="/publications">publications</a> page (let us know if you want us to add yours).</p>

<h4 class="question" id="where-do-you-announce-updates">Where do you announce updates?</h4>
<p>See <a href="https://github.com/marian-nmt/marian-dev/blob/master/CHANGELOG.md">changelog</a> for a curated list of changes or
follow us directly on twitter <a href="https://twitter.com/marian_nmt?ref_src=twsrc%5Etfw">@marian_nmt</a> for highlights.</p>

<h4 class="question" id="who-is-responsible-for-marian">Who is responsible for Marian?</h4>
<p>See the list of <a href="https://github.com/marian-nmt/marian-dev/graphs/contributors"><strong>marian-dev</strong> contributors</a> and <a href="https://github.com/marian-nmt/marian/graphs/contributors"><strong>marian</strong> contributors</a>.
As <a href="https://github.com/marian-nmt/marian-dev"><strong>marian-dev</strong></a> is our bleeding-edge development repository the main work on <strong>marian</strong> is happening there.
The <a href="https://github.com/marian-nmt/marian"><strong>marian</strong></a> repository is then updated with new versions.</p>

<p>Apart from that <a href="https://github.com/marian-nmt/marian"><strong>marian</strong></a> still contains code for <strong>amun</strong> our hand-written NMT decoder.
Contributions listed for that repository are mostly to <strong>amun</strong>.</p>

<p>The list of contributors so far:</p>

<ul>
  <li>Marcin Junczys-Dowmunt, formerly Adam Mickiewicz University in Poznań and University of Edinburgh</li>
  <li>Roman Grundkiewicz, University of Edinburgh and Adam Mickiewicz University in Poznań</li>
  <li>Hieu Hoang</li>
  <li>Tomasz Dwojak, Adam Mickiewicz University in Poznań, formerly University of Edinburgh</li>
  <li>Ulrich German, University of Edinburgh</li>
  <li>Alham Aji, University of Edinburgh</li>
  <li>Nikolay Bogoychev, University of Edinburgh</li>
  <li>Kenneth Heafield, University of Edinburgh</li>
  <li>Alexandra Birch, University of Edinburgh</li>
</ul>

<h4 class="question" id="whats-up-with-the-name">What’s up with the name?</h4>
<p>Marian has been named in honour of the Polish crypotologist <a href="https://en.wikipedia.org/wiki/Marian_Rejewski">Marian Rejewski</a>
who reconstructed the German military Enigma cipher machine in 1932.</p>

<p><a href="https://github.com/emjotde">Marcin</a> (the creator of the Marian toolkit) was born in the same Polish city as Marian Rejewski (Bydgoszcz), taught a bit of mathematics at Marian Rejewski’s
secondary school in Bydgoszcz and finally ended up studying mathematics at Adam Mickiewicz University in Poznań, at Marian Rejewski’s old faculty.</p>

<p>The name started out as a joke, but was made official later by public demand.</p>

<h3 id="training">Training</h3>

<h4 class="question" id="what-model-types-are-currently-available">What model types are currently available?</h4>

<h4 class="question" id="what-are-recommended-training-settings">What are recommended training settings?</h4>

<h4 class="question" id="how-do-i-enable-multi-gpu-training">How do I enable multi-GPU training?</h4>

<p>You only need to specify the device ids of the GPUs you want to use for training
(this also works with most other binaries) as <code class="highlighter-rouge">--devices 0 1 2 3</code> for training
on four GPUs. There is no automatic detection of GPUs for now.</p>

<p>By default, this will use asynchronous SGD (or rather ADAM). For the deeper models
and the transformer model, we found async SGD to be unreliable and you may want
to use a synchronous SGD variant by setting <code class="highlighter-rouge">--sync-sgd</code>.</p>

<p>For asynchronous SGD, the mini-batch size is used locally, i.e. <code class="highlighter-rouge">--mini-batch 64</code>
means 64 sentences per GPU worker.</p>

<p>For synchronous SGD, the mini-batch size is used
globally and will be divided across the number of workers. This means that for
synchronous SGD the effective mini-batch can be set N times larger for N GPUs. A
mini-batch size of <code class="highlighter-rouge">--mini-batch 256</code> will mean a mini-batch of 64 per worker if
four GPUs are used. This choice makes sense when you realize that synchronous
SGD is essentially working like a single GPU training process with N times more
memory. Larger mini-batches in a synchronous setting result in quite stable
training.</p>

<h4 class="question" id="how-do-i-chose-mini-batch-size-max-length-and-workspace-memory">How do I chose mini-batch size, max-length and workspace memory?</h4>
<p>Unfortunately this is quite involved and depends on the type of model, the available
GPU memory, the number of GPUs, a number of other parameters like the chosen
optimization algorithm, and the average or maximum sentence length in your
training corpus (which you should know!).</p>

<p>The options <code class="highlighter-rouge">--workspace 4000</code> sets the size of the memory available for the forward
and backward step of the training procedure. This does not include model size and
optimizer parameters that are allocated outsize workspace. Hence you cannot allocate
all GPU memory to workspace. If you are not happy with default values this is a trial and error
process.</p>

<p>Setting <code class="highlighter-rouge">--mini-batch 64 --max-length 100</code> will generate batches that contain
always 64 sentences (or less if the corpus is smaller) of up to a length of 100
tokens. Sentences longer than that are filtered out. Marian will grow workspace memory
if required and potentially exceed available memory, resulting in a crash. Workspace
memory is always rounded to multiples of 512 MB.</p>

<p><code class="highlighter-rouge">--mini-batch-fit</code> overrides the specified mini-batch size and automatically choses
the largest mini-batch for a given sentence length that fits the specified memory. When
<code class="highlighter-rouge">--mini-batch-fit</code> is set, memory requirements are guaranteed to fit into the specified
workspace. Choosing a too small workspace will result in small mini-batches which can prohibit
learning.</p>

<h5 id="my-rules-of-thumb">My rules of thumb:</h5>

<p>For shallow models I usually set the working memory to values between 3500 and 6000 (MB),
e.g. <code class="highlighter-rouge">--workspace 5500</code> and then use <code class="highlighter-rouge">--mini-batch-fit</code> which automatically tries to make
the best use of the specified memory size, mini-batch size and sentence length.</p>

<p>For very deep models, I first set all other parameters like <code class="highlighter-rouge">--max-length 100</code>, model type, depth etc.
Next I use <code class="highlighter-rouge">--mini-batch-fit</code> and try to max out <code class="highlighter-rouge">--workspace</code> until I get a crash due to insufficient memory. I then
revert to the last workspace size that did not crash. Since setting <code class="highlighter-rouge">--mini-batch-fit</code> guarantees that memory
will not grow during training due to batch-size this should result in a stable training run and maximal batch size.</p>

<h4 class="question" id="how-do-i-use-a-validation-set">How do I use a validation set?</h4>

<p>Just provide <code class="highlighter-rouge">--valid valid.src valid.trg</code>. Be default this provide sentence-wise
normalized cross-entropy scores for the validation set every 10,000 iterations.
You can change the validation frequency to, say 5000, with <code class="highlighter-rouge">--valid-freq 5000</code> and
the display frequency to 500 with <code class="highlighter-rouge">--disp-freq 500</code>.</p>

<p><strong>Attention:</strong> the validation set needs to have been preprocessed in exactly the same
manner as your training data.</p>

<h4 class="question" id="what-types-of-validation-scores-are-available">What types of validation scores are available?</h4>
<p>Be default we report sentence-wise normalized cross-entropy, but you can specify
different and more than one metrics, for example <code class="highlighter-rouge">--valid-metrics perplexity ce-mean-words translation</code>
which will report word-wise normalized perplexity, word-wise normalized cross-entropy and will
run in-process translation of the validation set to be scored with an external validation script.
See <a href="/docs/#validation">this part of the documentation</a> for more details.</p>

<h4 class="question" id="how-long-do-i-need-to-train-my-models">How long do I need to train my models?</h4>
<p>This is a difficult question. What I usually do as a rule of thumb is to use a
validation set as described above and the default settings for <code class="highlighter-rouge">--early-stopping 10</code>.
This means that training will finish if the first specified metric in <code class="highlighter-rouge">--valid-metrics</code>
did not improve (stalled) for 10 consecutive validation steps. Usually this will
signal convergence or — if the scores get worse with later validation steps —
potential overfitting.</p>

<h4 class="question" id="what-types-of-regularization-are-available">What types of regularization are available?</h4>
<p>The numeric values in this answer are only provided as examples.</p>

<p>Depending on the model type, Marian support multiple types of dropout. For
RNN-based models it supports the <code class="highlighter-rouge">--dropout-rnn 0.2</code> option which uses
variational dropout on all RNN inputs and recursive states.</p>

<p>There is also <code class="highlighter-rouge">--dropout-src 0.1</code> and <code class="highlighter-rouge">--dropout-trg 0.1</code> which drops out entire
source and target word positions, respectively. This is an options we found to
be useful for monolingual settings.</p>

<p>For the transformer model the equivalent of <code class="highlighter-rouge">--dropout-rnn 0.2</code> is <code class="highlighter-rouge">--transformer-dropout 0.2</code>.</p>

<p>Apart from dropout, we also provide <code class="highlighter-rouge">--label-smoothing 0.1</code> as suggested by <a href="https://arxiv.org/abs/1706.03762">Vaswani et
  al., 2017</a>.</p>

<h4 class="question" id="how-do-i-train-a-google-style-transformer-model">How do I train a Google-style transformer model?</h4>

<p>Please take a look at our <a href="https://github.com/marian-nmt/marian-examples/blob/master/transformer/README.md">transformer example</a>.
Files and scripts in this folder show how to train a Google-style transformer model
<a href="https://arxiv.org/abs/1706.03762">Vaswani et al, 2017</a> on WMT-17 (?) English-German data.
The problem-set has been adapted from the original tensor2tensor repository by Google.
We reuse their 36,000 common BPE subword units for both languages.
No back-translationed data was added.</p>

<p>The memory setting was adapted for a 12GB Titan X. If you are using GPUs with more
RAM, e.g. a Volta with 16GB, you can set <code class="highlighter-rouge">-w 13000</code> instead of <code class="highlighter-rouge">-w 7000</code>. If you train models
for ensembling remember to change the seed between training runs to initialize
models differently.</p>

<h4 class="question" id="how-do-i-train-a-model-like-in-edinburghs-wmt2017-submission">How do I train a model like in Edinburgh’s WMT2017 submission?</h4>

<p>Re-using the transformer example from above, you can train a model similar to
<a href="http://www.statmt.org/wmt17/pdf/WMT39.pdf">Edinburgh’s WMT2017 submission</a> with the following settings:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>../build/marian \
    --model model/model.npz --type s2s \
    --train-sets data/corpus.bpe.en data/corpus.bpe.de \
    --max-length 100 \
    --vocabs model/vocab.ende.yml model/vocab.ende.yml \
    --mini-batch-fit -w 7000 --maxi-batch 1000 \
    --early-stopping 10 \
    --beam-size 6 --normalize 0.6 \
    --log model/train.log --valid-log model/valid.log \
    --enc-type bidirectional --enc-depth 1 --enc-cell-depth 4 \
    --dec-depth 1 --dec-cell-base-depth 8 --dec-cell-high-depth 1 \
    --tied-embeddings-all --layer-normalization \
    --dropout-rnn 0.1 --label-smoothing 0.1 \
    --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report \
    --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 \
    --tied-embeddings-all \
    --devices $GPUS --sync-sgd --seed 1111 \
    --valid-freq 5000 --save-freq 5000 --disp-freq 500 \
    --valid-metrics cross-entropy perplexity translation \
    --valid-sets data/valid.bpe.en data/valid.bpe.de \
    --valid-script-path ./scripts/validate.sh \
    --valid-translation-output data/valid.bpe.en.output --quiet-translation \
    --valid-mini-batch 64
</code></pre>
</div>

<p>The variable $GPUS would hold the GPU ids, for instance GPUS=”0 1 2 3”. As before
you can increase the workspace if more GPU RAM is available. If you train models
for ensembling remember to change the seed between training runs to initialize
models differently.</p>

<p>We will add an example to our examples repository.</p>

<h4 class="question" id="how-do-i-train-a-language-model">How do I train a language model?</h4>

<h4 class="question" id="how-do-i-train-a-multi-source-model">How do I train a multi-source model?</h4>

<h3 id="translation">Translation</h3>

<h4 class="question" id="are-there-recommended-settings-for-translation">Are there recommended settings for translation?</h4>
<p>We found that using length normalization with a penalty term of 0.6 and a beam size of 6 is usually best. This rougly follows the settings by Google from
their <a href="https://arxiv.org/abs/1706.03762">transformer paper</a>.
Assuming your model file is <code class="highlighter-rouge">model.npz</code> and your vocabulary is <code class="highlighter-rouge">vocab.src.yml</code> and <code class="highlighter-rouge">vocab.trg.yml</code>,
we recommend to use the following options:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./marian-decoder -m model.npz -v vocab.src.yml vocab.trg.yml -b 6 --normalize=0.6
</code></pre>
</div>

<h4 class="question" id="does-marian-support-batched-translation">Does Marian support batched translation?</h4>

<p>Yes. This a feature introduced in Marian v1.1.0. Batched translation generates translation for whole mini-batches and significantly increases
translation speed (roughly by a factor of 10 or more). See <a href="/docs/#batched-translation">this documentation section</a> for details.</p>

<h4 class="question" id="can-marian-do-model-ensembling">Can Marian do model ensembling?</h4>

<p>Yes, and you can even ensemble models of different types, for instance an Edinburgh-style deep RNN model and a Google Transformer model, or you can a
language model to the mix. See <a href="/docs/#model-ensembling">here</a> for details.</p>

<h4 class="question" id="are-there-options-to-deal-with-placeholders-or-xml-tags">Are there options to deal with placeholders or XML-tags?</h4>
<p>No. This is a difficult issue for neural machine translation and we did not have the man power or motivation yet to deal with this.</p>

<h4 class="question" id="can-i-access-the-attention-weights">Can I access the attention weights?</h4>
<p>Yes and no. <code class="highlighter-rouge">amun</code> has options for this, but is restricted to a specific model type. <code class="highlighter-rouge">marian-decoder</code> does not provide this options yet,
but we are open to adding it if there is demand.</p>

<h4 class="question" id="can-i-generate-n-best-lists">Can I generate n-best lists?</h4>
<p>Yes. Just use <code class="highlighter-rouge">--n-best</code> and the set <code class="highlighter-rouge">--beam-size 6</code> for an n-best list size of 6.</p>

<h3 id="scoring">Scoring</h3>

<h4 class="question" id="how-can-i-calculate-the-perplexity-of-a-test-set">How can I calculate the perplexity of a test set?</h4>
<p>Assuming your model file is <code class="highlighter-rouge">model.npz</code> and your vocabulary is <code class="highlighter-rouge">vocab.src.yml</code> and <code class="highlighter-rouge">vocab.trg.yml</code> and
your test set files are <code class="highlighter-rouge">test.src</code> and <code class="highlighter-rouge">test.trg</code> use the following command:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./marian-scorer -m model.npz -v vocab.src.yml vocab.trg.yml -t test.src test.trg --summary=perplexity
</code></pre>
</div>

<p>If your model had a different number of inputs (only one for a language model or three for a dual-source model)
you need to provide all the correct number of vocabularies and test set files in corresponding order.</p>

<p>Omitting the <code class="highlighter-rouge">--summary</code> option will print sentence-wise negative log probabilities.</p>

<h4 class="question" id="are-moses-style-n-best-lists-supported">Are Moses-style n-best lists supported?</h4>
<p>Not yet. If you want to use the rescorer for for n-best list rescoring you need to extract the sentences to a
plain text file. If the model is a translation model you also need to create a source file that has the correct
source sentences in the right order and number, i.e. you need to repeat the source sentence as many times as there
are entries in the corresponding n-best list.</p>

<!--
### Training

System | 2013 | 2014 | 2015 | 2016
-----------|--------|---------|--------|-------
Edinburgh Deep RNN ([Micelli Barone et al 2017](https://arxiv.org/pdf/1707.07631.pdf)) | - | 23.4 | 26.0 | 31.0
Transformer 12-layers ([Ramachandran et al. 2017](https://arxiv.org/pdf/1710.05941.pdf)) | 26.1* | 27.8* | 29.8* | 33.3*
**Marian Transformer 6-layers** (epoch 27!) | 25.5* | 26.7 | 29.5 | 33.2
**Marian Transformer 6-layers** (epoch 43!) | 25.5* | 26.9 | 29.2 | 33.4
**Marian Edinburgh Deep RNN** (epoch 9) | 24.8* | 24.9 | 28.4 | 32.7
**Marian Edinburgh Deep RNN** (epoch 16) | 25.0* | 25.2 | 28.4 | 32.6

-->


            </div><!--//content-inner-->
          </div><!--//doc-content-->

          <div class="doc-sidebar hidden-xs">
            <nav id="doc-nav"></nav>
          </div><!--//doc-sidebar-->

        </div><!--//doc-body-->

      </div><!--//container-->
    </div><!--//doc-wrapper-->

    </div><!--//page-wrapper-->

    <footer id="footer" class="footer text-center">
  <div class="container">
    <p>
     Marian - an efficient Neural Machine Translation framework written in pure C++.</br>
      Mainly developed at the Adam Mickiewicz University in Poznań and at the University of Edinburgh.
    </p>
    <p><a href="https://github.com/marian-nmt/marian">Marian</a> is licensed under the <a href="https://github.com/marian-nmt/marian/LICENSE">MIT license</a>.</p>
    <small class="copyright">Based on the theme PrettyDocs designed by <a href="http://themes.3rdwavemedia.com/" targe="_blank">Xiaoying Riley</a> with modifications.</small>
  </div><!--//container-->
</footer><!--//footer-->

    <!-- Main Javascript -->
<script type="text/javascript"> localStorage.clear();</script>

<script type="text/javascript" src="/assets/plugins/jquery-1.12.3.min.js"></script>
<script type="text/javascript" src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-scrollTo/jquery.scrollTo.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-match-height/jquery.matchHeight-min.js"></script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/javascript" src="/assets/js/main.js"></script>
<script type="text/javascript" src="/assets/js/toc.js"></script>


  </body>
</html>
